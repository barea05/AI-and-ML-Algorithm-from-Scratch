{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9259c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b9eeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>0</td>\n",
       "      <td>wall street cool to ebay s profit shares in on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0</td>\n",
       "      <td>ban on forced retirement under 65 employers wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1</td>\n",
       "      <td>time to get tough on friendlies  for an intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>0</td>\n",
       "      <td>christmas shoppers flock to tills shops all ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>0</td>\n",
       "      <td>bush budget seeks deep cutbacks president bush...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1017 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "0            0  worldcom boss  left books alone  former worldc...\n",
       "1            1  tigers wary of farrell  gamble  leicester say ...\n",
       "2            1  yeading face newcastle in fa cup premiership s...\n",
       "3            1  henman hopes ended in dubai third seed tim hen...\n",
       "4            1  wilkinson fit to face edinburgh england captai...\n",
       "...        ...                                                ...\n",
       "1012         0  wall street cool to ebay s profit shares in on...\n",
       "1013         0  ban on forced retirement under 65 employers wi...\n",
       "1014         1  time to get tough on friendlies  for an intern...\n",
       "1015         0  christmas shoppers flock to tills shops all ov...\n",
       "1016         0  bush budget seeks deep cutbacks president bush...\n",
       "\n",
       "[1017 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'logistic_regression_assignment_data.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b228303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49559998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    509\n",
       "0    508\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c21b66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train_Text =  (813,)\n",
      "Shape of Test_Text =  (204,)\n",
      "Shape of Train_Category =  (813,)\n",
      "Shape of Test_Category =  (204,)\n"
     ]
    }
   ],
   "source": [
    "text = df['text']\n",
    "category = df['category']\n",
    "\n",
    "train_text, test_text, train_category, test_category = train_test_split(text, category, test_size= 0.2, random_state = 42, stratify= category)\n",
    "print(\"Shape of Train_Text = \", train_text.shape)\n",
    "print(\"Shape of Test_Text = \", test_text.shape)\n",
    "print(\"Shape of Train_Category = \", train_category.shape)\n",
    "print(\"Shape of Test_Category = \", test_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a412fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3), max_features=2000, min_df = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64bdf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((813, 2000), (204, 2000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors = vectorizer.fit_transform(train_text)\n",
    "test_vectors = vectorizer.transform(test_text)\n",
    "train_vectors.shape, test_vectors.shape\n",
    "\n",
    "#test ony contains .transform mrthod\n",
    "#train always contains fit.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382684ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000 jobs', '000 people', '12 months', ..., 'you have',\n",
       "       'you have to', 'you need'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bc00dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = train_vectors.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8a850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler(with_mean=False, with_std=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False, with_std=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler(with_mean=False, with_std=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False,with_std=False)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb9b86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((813, 2000), (204, 2000))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors_stand = scaler.fit_transform(train_vectors)\n",
    "test_vectors_stand =  scaler.transform(test_vectors)\n",
    "train_vectors_stand.shape,test_vectors_stand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb17cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.title(\"Standard data\")\n",
    "#plt.plot('train_vectors_stand[0].tolist')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab74880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_vectors_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b48229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_bias(dim):\n",
    "    ''' In this function, we will initialize our weights and bias terms'''\n",
    "    \n",
    "\n",
    "    # Initialize the weights to zeros array of (dim) dimensions. Here dim will be the number of features of your tfidf vectorizer output.\n",
    "    # You can initialize the weight terms with zeros.\n",
    "    # Initialize bias term to zero\n",
    "    # Write your code below.\n",
    "    w = np.zeros(dim)\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55d1b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 0., ..., 0., 0., 0.]), 0)\n"
     ]
    }
   ],
   "source": [
    "weight_init = initialize_weights_bias(dim)\n",
    "print(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddec3161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grader_1 Status :  True\n"
     ]
    }
   ],
   "source": [
    "# Grader function to check the initialization of your weights and bias terms.\n",
    "def grader_weights_bias(w,b):\n",
    "  assert((len(w)==2000) and b==0)\n",
    "  return True\n",
    "\n",
    "dim = 2000\n",
    "w,b = initialize_weights_bias(dim)\n",
    "grader_1 = grader_weights_bias(w,b)\n",
    "print(\"Grader_1 Status : \", grader_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d17819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    \n",
    "    # Compute sigmoid(z) and return its value.\n",
    "    # Write your code below.\n",
    "    \n",
    "    sigmoid = 1/(1 + np.exp(-z))\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669c3bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grader_2 Status :  True\n"
     ]
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val = custom_sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "\n",
    "grader_2 = grader_sigmoid(2)\n",
    "print(\"Grader_2 Status : \", grader_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505d150",
   "metadata": {},
   "source": [
    "\n",
    "ùëôùëúùëîùëôùëúùë†ùë† =  ‚àí1‚àó1ùëõŒ£ùëìùëúùëüùëíùëéùëê‚Ñéùëåùë°ùëüùë¢ùëí,ùëåùëùùëüùëíùëë(ùëåùë°ùëüùë¢ùëíùëôùëúùëî10(ùëåùëùùëüùëíùëë)+(1‚àíùëåùë°ùëüùë¢ùëí)ùëôùëúùëî10(1‚àíùëåùëùùëüùëíùëë)) \n",
    "\n",
    "\n",
    "ùêø1ùëôùëúùë†ùë†=Œ£ùëìùëúùëüùëíùëéùëê‚Ñéùë§(|ùë§|)\n",
    "\n",
    "ùë°ùëúùë°ùëéùëôùëôùëúùë†ùë†=ùëôùëúùëîùëôùëúùë†ùë†+ùëéùëôùëù‚Ñéùëé‚àóùêø1ùëôùëúùë†ùë†\n",
    "\n",
    "Where alphas is the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e863df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07cce4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absloute_value(values):\n",
    "    l1_loss = 0\n",
    "    for v in values:\n",
    "        l1_loss += abs(v)\n",
    "    return l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1597f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, alpha, weights):\n",
    "    '''In this function, we will compute total loss which is [(logloss) + (alpha * L1regularization loss)] '''\n",
    "    \n",
    "    # Write your code below.\n",
    "    \n",
    "    log_loss = 0\n",
    "    n = len(y_true)\n",
    "    \n",
    "    for y_true, y_pred in zip(y_true,y_pred):\n",
    "         log_loss += y_true*np.log10(y_pred)+(1-y_true)*log10(1-y_pred)\n",
    "    \n",
    "    log_loss = -log_loss/n\n",
    "    \n",
    "    l1_loss = absloute_value(weights)\n",
    " \n",
    "    total_loss = log_loss + (alpha*l1_loss) \n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "158bceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grader_3 Status :  True\n"
     ]
    }
   ],
   "source": [
    "# Grader function to check the implementaiton of logloss\n",
    "\n",
    "def grader_loss():\n",
    "    true_values = [1,1,0,1,0]\n",
    "    pred_values = [0.9,0.8,0.1,0.8,0.2]\n",
    "    w= np.array([0.1]*10)\n",
    "    alpha= 0.0001\n",
    "    loss = custom_loss(true_values, pred_values,alpha,w)\n",
    "    assert(loss == 0.07644900402910389+0.0001*10*0.1)\n",
    "    return True\n",
    "\n",
    "\n",
    "grader_3 = grader_loss()\n",
    "print(\"Grader_3 Status : \", grader_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b046400",
   "metadata": {},
   "source": [
    "ùêøùëñ = ‚àí(ùëåùëñùëôùëúùëî10(ùùàùëñ) ‚àí (1‚àíùëåùëñ)ùëôùëúùëî10(1‚àíùùàùëñ) + ùëéùëôùëù‚Ñéùëé/ùëÅ*(ùë†ùë¢ùëö(|ùë§|)) \n",
    "\n",
    "ùëä‚Ñéùëíùëüùëí:ùùàùëñ = œÉ(ùë§ùëáùë•ùëñ+ùëè) \n",
    "\n",
    "And: L1 regularization =  ùëéùëôùëù‚Ñéùëé/ùëÅ*(ùë†ùë¢ùëö(|ùë§|))\n",
    "\n",
    "Alpha: It is the Regularization parameter\n",
    "\n",
    "N : number of training examples\n",
    "\n",
    "œÉ : sigmoid function <b\n",
    "\n",
    "ùëëùêøùëñ/ùëëùë§ = ‚àíùëåùëñ ùë•ùëñ(1‚àíùùàùëñ) + (1‚àíùëåùëñ)ùë•ùëñùùàùëñ + (ùëéùëôùëù‚Ñéùëé/ùëÅ)*( (ùë§ + (1ùëí‚àí5)) / |ùë§+(1ùëí‚àí5)| )\n",
    "\n",
    "                        \n",
    "NOTE THAT: 1e-5 used in numerator and denominator to avoid division error\n",
    "\n",
    "ùëëùêøùëñ/ùëëùëè = ‚àíùëåùëñ(1‚àíùùàùëñ) + (1‚àíùëåùëñ)ùùàùëñ \n",
    "\n",
    "\n",
    "Hence,\n",
    "                        \n",
    "ùëëùêøùëñ/ùëëùë§ = ùëëùë§ = (ùùàùëñ‚àíùëåùëñ)ùë•ùëñ + ùëéùëôùëù‚Ñéùëé/ùëÅ * ( (ùë§ + (1ùëí‚àí5))|ùë§ + (1ùëí‚àí5)|) ) \n",
    "\n",
    "1e-5 used in numerator and denominator to avoid division error\n",
    "\n",
    "ùëëùêøùëñ/ùëëùëè = ùëëùëè = ùùàùëñ‚àíùëåùëñ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b714db",
   "metadata": {},
   "source": [
    "z =2\n",
    "w,b = initialize_weights_bias(dim)\n",
    "\n",
    "alpha = 0.0001\n",
    "N = 2000                                                       \n",
    "x = np.array(train_vectors)\n",
    "y = np.array(df[\"category\"])                                               \n",
    "gradient_dw(x, y, w, b, alpha, N)\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "\n",
    "dw = x*(y - sigmoid(z)) - ((1/alpha)*(1/N) * w)\n",
    "\n",
    "\n",
    "dLi/dw = dw= (œÉi‚àíYi)xi + alphaNw + (1e‚àí5)|w+(1e‚àí5)| \n",
    "\n",
    "1e-5 used in numerator and denominator to avoid division error\n",
    "\n",
    "dLi/db=db=œÉi‚àíYi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ec75861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_dw(x, y, w, b, alpha, N):\n",
    "    '''In this function, we will compute the gardient w.r.t. w '''\n",
    "    # Write your code below.\n",
    "    dw = ((custom_sigmoid(np.dot(x, w) + b) - y) *  x) + ((alpha/N) * (w + 1e-5 / abs(w + 1e-5)))\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a9ab767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_db(x, y, w, b):\n",
    "    '''In this function, we will compute the gardient w.r.t. b '''\n",
    "    # Write your code below.\n",
    "    db = custom_sigmoid(np.dot(w,x) + b) - y\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2aede4",
   "metadata": {},
   "source": [
    "  Implement the code as follows:\n",
    "    \"\"\"\n",
    "  In this function we will compute optimal values for weights and bias terms on\n",
    "  the train data. \n",
    "\n",
    "  Here eta0 is the learning rate and alpha is the regularization term.\n",
    "  \"\"\"\n",
    "\n",
    "  1. Initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "  2. Repeat For many epochs until condition \"e\"  fails\n",
    "          # a) for every data point(X_train,y_train)\n",
    "                # compute gradient w.r.to w (call the gradient_dw() function)\n",
    "                # compute gradient w.r.to b (call the gradient_db() function)\n",
    "                # update w, b using the above eqns\n",
    "          # b) predict the output of x_train[for all data points in X_train] using w,b\n",
    "          # c) compute the loss between predicted and actual values (call the loss function)\n",
    "          # d) store all the train loss values in a list\n",
    "          # e) Compare previous loss and current loss, if the difference between loss is not more than or equal to the tolerance, stop the process and return w,b\n",
    "\n",
    "   3. Return the values of weights, bias, train_loss and num_epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d964089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train(X_train, y_train, alpha, eta0,tolerance):\n",
    "    N = X_train.shape[1]\n",
    "    w, b = initialize_weights_bias(N)\n",
    "    # *((w + (np.exp(-5))) / (w + abs(np.exp(-5))))   #w+(1e‚àí5)|\n",
    "    epoch = 200\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            dw = gradient_dw(x, y, w, b, alpha, N)\n",
    "            db = gradient_db(x, y, w, b)\n",
    "            w = w - eta0 * dw\n",
    "            b = b - eta0 * db\n",
    "        y_pred = [custom_sigmoid(np.dot(w, x)) for x in X_train]\n",
    "        train_loss = custom_loss(y_train, y_pred, alpha, w)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        if len(train_losses)>=2 and train_losses[-2] - train_losses[-1]  <  tolerance:\n",
    "              break\n",
    "\n",
    "        print(f\"Epoch: {epoch} Train Loss: {train_loss}\")\n",
    "\n",
    "    return w, b, train_losses, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af5ecf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grader_weights_bias():\n",
    "    clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, \n",
    "                                     loss='log_loss', random_state=15, \n",
    "                                     penalty='l1', tol=1e-3, \n",
    "                                     learning_rate='constant'\n",
    "                                    )\n",
    "    clf.fit(train_vectors_stand,train_category.values)\n",
    "    model_coef = clf.coef_[0]\n",
    "\n",
    "    # fitting custom train with same learning rate, regularization and tolerance as of sklearn\n",
    "    w, b, _, epoch = custom_train(train_vectors_stand.toarray(), train_category.values, 0.0001,0.0001,1e-3)\n",
    "\n",
    "    # checking whether the weights and bias returned by both the implementations are closer\n",
    "    print(f\"Weights: {w}\")\n",
    "    print(f\"SGD Weights: {model_coef}\")\n",
    "    print(f\"Bias {b} SGB Bias {clf.intercept_}\")\n",
    "    \n",
    "    for wdiff in (w - model_coef):\n",
    "        #print(f\"Weights Difference: {wdiff}\")\n",
    "        if wdiff > 0.02:\n",
    "            print(f\"----------{wdiff}\")\n",
    "    \n",
    "    assert(not (b - clf.intercept_>0.02)==True)\n",
    "    assert((not (w - model_coef>0.02).any())==True)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e31441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.30091950891663255\n",
      "Weights: [-2.60285681e-04 -1.88837739e-04 -1.80145788e-05 ...  2.31933405e-04\n",
      "  1.57175797e-04  1.14236868e-04]\n",
      "SGD Weights: [-7.31962530e-04 -5.19249656e-04 -5.42397981e-06 ...  6.46917855e-04\n",
      "  4.22730730e-04  2.94555212e-04]\n",
      "Bias 9.411542029356958e-05 SGB Bias [3.13505211e-06]\n",
      "Grader_4 Status :  True\n"
     ]
    }
   ],
   "source": [
    "grader_4 = grader_weights_bias()\n",
    "print(\"Grader_4 Status : \", grader_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c8d8a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.30091950891663255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting graph for epoch vs loss for train and test data\n",
    "\n",
    "w,b,train_loss,epochs = custom_train(train_vectors_stand.toarray(), train_category.values, 0.0001,0.0001,0.001)\n",
    "plt.plot(range(epochs+1), train_loss, label='train curve')\n",
    "plt.title('epoch vs loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8eae174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b, X):\n",
    "    \n",
    "    '''function to predict label given weights, bias and standardized data'''\n",
    "    \n",
    "    #m = X.shape[1]\n",
    "    #Y_prediction = np.zeros((1, m))\n",
    "    #w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        z = np.dot(w, x) + b\n",
    "        prediction = custom_sigmoid(z)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return np.array(predictions) #it should be a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6c87758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.30091950891663255\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def grader_predict():\n",
    "    ''' grader to check the test accuracy'''\n",
    "    \n",
    "    w, b, _, _ = custom_train(train_vectors_stand.toarray(), train_category.values, 0.0001,0.0001,0.001)\n",
    "    \n",
    "    test_preds = predict(w, b, test_vectors_stand.toarray())\n",
    "    \n",
    "    test_preds = [ 1 if test >= 0.5 else 0 for test in test_preds]\n",
    "    \n",
    "    test_accuracy = (np.sum(test_category == test_preds)/len(test_preds))*100\n",
    "    \n",
    "    if (test_accuracy >= 90):\n",
    "        print(\"Success!\")\n",
    "    else:\n",
    "        print(\"Failed! \\n Test accuracy = \", test_accuracy)\n",
    "    return\n",
    "  \n",
    "grader_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbb42575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_v2(X_train, y_train,alpha, eta0, tolerance):\n",
    "    \"\"\"\n",
    "    In this function we will compute optimal values for weights and bias terms on\n",
    "    the train data. \n",
    "\n",
    "    Here eta0 is the learning rate and alpha is the regularization term.\n",
    "    \"\"\"\n",
    "    #array to store train loss for each epoch\n",
    "    train_loss=[]\n",
    "\n",
    "  # Implement the code as follows:\n",
    "\n",
    "  # 1. Initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    \n",
    "    \n",
    "  # 2. Repeat For many epochs until condition \"e\"  fails\n",
    "          # a) for every data point(X_train,y_train)\n",
    "                # compute gradient w.r.to w (call the gradient_dw() function)\n",
    "                # compute gradient w.r.to b (call the gradient_db() function)\n",
    "                # update w, b using the above eqns\n",
    "          # b) predict the output of x_train[for all data points in X_train] using w,b\n",
    "          # c) compute the loss between predicted and actual values (call the loss function)\n",
    "          # d) store all the train loss values in a list\n",
    "          # e) Compare previous loss and current loss, if the difference between loss is not more than or equal to the tolerance, stop the process and return w,b\n",
    "\n",
    "  # 3. Return the values of weights, bias, train_loss and num_epochs \n",
    "  \n",
    "  #initializing the weights and bias\n",
    "    w,b=initialize_weights_bias(X_train.shape[1])\n",
    "\n",
    "  #storing the number of train points in N\n",
    "    N=len(X_train)\n",
    "\n",
    "  #number of epochs the model is trained\n",
    "    num_epochs=0\n",
    "\n",
    "  #condition to run the training for more epochs\n",
    "    cond=True\n",
    "\n",
    "  #checking whether to run the training for more epochs\n",
    "    while(cond):\n",
    "    \n",
    "    #looping through each datapoint\n",
    "    \n",
    "        for j in range(len(X_train)):\n",
    "            x=X_train[j]\n",
    "            y=y_train[j]\n",
    "\n",
    "            #computing gradients\n",
    "            dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "            db=gradient_db(x,y,w,b)\n",
    "\n",
    "            #updating weights\n",
    "            w=w-eta0*dw\n",
    "            b=b-eta0*db\n",
    "\n",
    "    #computing y_pred for entire train data as y_pred= sigmoid(wTx+b)\n",
    "        y_pred_train = np.array([custom_sigmoid(np.dot(w,X_traini)+b) for X_traini in X_train])\n",
    "\n",
    "    #adding the train loss of current epoch to the list\n",
    "        train_loss.append(custom_loss(y_train,y_pred_train,alpha,w))\n",
    "\n",
    "    #checking whether loss has improved from previous epoch by atleast tolerance\n",
    "    \n",
    "        if num_epochs>0:\n",
    "            cond = ((train_loss[num_epochs - 1] - train_loss[num_epochs]) >= tolerance)\n",
    "\n",
    "    #increasing the number of epochs the model is trained.\n",
    "        num_epochs=num_epochs + 1\n",
    "\n",
    "    #decreasing learning rate by 10% each epoch\n",
    "        eta0 = eta0*0.90  \n",
    "\n",
    "    return w,b,train_loss,num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a407d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, n = custom_train_v2(train_vectors_stand.toarray(), train_category.values, 0.0001,0.0001,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dae8172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.47276510e-04 -1.79402928e-04 -1.71369523e-05 ...  2.20339193e-04\n",
      "  1.49317656e-04  1.08524353e-04]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac2657f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.96738752418314e-05\n"
     ]
    }
   ],
   "source": [
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab98d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
